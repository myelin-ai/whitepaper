Second generation neural networks achieve impressive results
on all tasks that can collectively be identified as data classification.
As of the time of writing, second generation convolutional networks
continue dominating every ImageNet classification challenge of the last
years~\cite{ILSVRC15}.

There are still types of tasks however that no deep learning topology or 
technique has been able to tackle so far. Adversarial evaluations of reading comprehension
systems indicate that neural networks that excell at evaluating carefully
prepared data prove to be extremely fragile when encountering organic ``noise''
information~\cite{DBLP:journals/corr/JiaL17}.
The practice of splitting learning into separate training and application steps
means that a traditional neural network is unable to continuously learn while working.
This way, new information cannot be aquired dynamically as it has to be spoon-fed in the
form of carefully prepared test data.
Further limitations of current deep learning strategies have been compiled by 
Gary Marcus~\cite{DBLP:journals/corr/abs-1801-00631}.

One way to interpret these limitations is as an expression of the growing discrepancy between
biological neural networks and the mentioned systems~\cite{Paugam-Moisy2012}.
